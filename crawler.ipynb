{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/the/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/the/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: selenium in /Users/the/Library/Python/3.11/lib/python/site-packages (4.20.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/homebrew/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/the/Library/Python/3.11/lib/python/site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/the/Library/Python/3.11/lib/python/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/homebrew/lib/python3.11/site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /opt/homebrew/lib/python3.11/site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/the/Library/Python/3.11/lib/python/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/the/Library/Python/3.11/lib/python/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/homebrew/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Requirement already satisfied: outcome in /Users/the/Library/Python/3.11/lib/python/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/homebrew/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/the/Library/Python/3.11/lib/python/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Users/the/Library/Python/3.11/lib/python/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/homebrew/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ban-nha-mat-pho-duong-bui-thi-xuan-phuong-bui-thi-xuan/ban-mp-vip-o-to-tranh-via-he-kinh-doanh-1-so-1-chu-no-hau-vuong-phong-thuy-pr39846461\n"
     ]
    }
   ],
   "source": [
    "# Set up Selenium WebDriver (you need to have appropriate drivers installed)\n",
    "driver = webdriver.Chrome()  # You can use other browsers and corresponding drivers too\n",
    "\n",
    "# Load the page\n",
    "driver.get(\"https://batdongsan.com.vn/ban-nha-dat-ha-noi\")\n",
    "\n",
    "# Wait for the page to load completely (you might need to adjust the time)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "# Get the page source after JavaScript has rendered the content\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Now you can use BeautifulSoup to parse the page source\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "# Find the link\n",
    "anchor_tags = soup.findAll('a', 'js__product-link-for-product-id')\n",
    "anchor_tag = anchor_tags[0]\n",
    "\n",
    "# Extract the href attribute\n",
    "if anchor_tag:\n",
    "    href = anchor_tag.get('href')\n",
    "    print(href)\n",
    "else:\n",
    "    print(\"No anchor tag found.\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####Done page 861\n",
      "####Done page 862\n",
      "####Done page 863\n",
      "####Done page 864\n",
      "####Done page 865\n",
      "####Done page 866\n",
      "####Done page 867\n",
      "####Done page 868\n",
      "####Done page 869\n",
      "####Done page 870\n",
      "####Done page 871\n",
      "####Done page 872\n",
      "####Done page 873\n",
      "####Done page 874\n",
      "####Done page 875\n",
      "####Done page 876\n",
      "####Done page 877\n",
      "####Done page 878\n",
      "####Done page 879\n",
      "####Done page 880\n",
      "####Done page 881\n",
      "####Done page 882\n",
      "####Done page 883\n",
      "####Done page 884\n",
      "####Done page 885\n",
      "####Done page 886\n",
      "####Done page 887\n",
      "####Done page 888\n",
      "####Done page 889\n",
      "####Done page 890\n",
      "####Done page 891\n",
      "####Done page 892\n",
      "####Done page 893\n",
      "####Done page 894\n",
      "####Done page 895\n",
      "####Done page 896\n",
      "####Done page 897\n",
      "####Done page 898\n",
      "####Done page 899\n",
      "####Done page 900\n",
      "####Done page 901\n",
      "####Done page 902\n",
      "####Done page 903\n",
      "####Done page 904\n",
      "####Done page 905\n",
      "####Done page 906\n",
      "####Done page 907\n",
      "####Done page 908\n",
      "####Done page 909\n",
      "####Done page 910\n",
      "####Done page 911\n",
      "####Done page 912\n",
      "####Done page 913\n",
      "####Done page 914\n",
      "####Done page 915\n",
      "####Done page 916\n",
      "####Done page 917\n",
      "####Done page 918\n",
      "####Done page 919\n",
      "####Done page 920\n",
      "####Done page 921\n",
      "####Done page 922\n",
      "####Done page 923\n",
      "####Done page 924\n",
      "####Done page 925\n",
      "####Done page 926\n",
      "####Done page 927\n",
      "####Done page 928\n",
      "####Done page 929\n",
      "####Done page 930\n",
      "####Done page 931\n",
      "####Done page 932\n",
      "####Done page 933\n",
      "####Done page 934\n",
      "####Done page 935\n",
      "####Done page 936\n",
      "####Done page 937\n",
      "####Done page 938\n",
      "####Done page 939\n",
      "####Done page 940\n",
      "####Done page 941\n",
      "####Done page 942\n",
      "####Done page 943\n",
      "####Done page 944\n",
      "####Done page 945\n",
      "####Done page 946\n",
      "####Done page 947\n",
      "####Done page 948\n",
      "####Done page 949\n",
      "####Done page 950\n",
      "####Done page 951\n",
      "####Done page 952\n",
      "####Done page 953\n",
      "####Done page 954\n",
      "####Done page 955\n",
      "####Done page 956\n",
      "####Done page 957\n",
      "####Done page 958\n",
      "####Done page 959\n",
      "####Done page 960\n",
      "####Done page 961\n",
      "####Done page 962\n",
      "####Done page 963\n",
      "####Done page 964\n",
      "####Done page 965\n",
      "####Done page 966\n",
      "####Done page 967\n",
      "####Done page 968\n",
      "####Done page 969\n",
      "####Done page 970\n",
      "####Done page 971\n",
      "####Done page 972\n",
      "####Done page 973\n",
      "####Done page 974\n",
      "####Done page 975\n",
      "####Done page 976\n",
      "####Done page 977\n",
      "####Done page 978\n",
      "####Done page 979\n",
      "####Done page 980\n",
      "####Done page 981\n",
      "####Done page 982\n",
      "####Done page 983\n",
      "####Done page 984\n",
      "####Done page 985\n",
      "####Done page 986\n",
      "####Done page 987\n",
      "####Done page 988\n",
      "####Done page 989\n",
      "####Done page 990\n",
      "####Done page 991\n",
      "####Done page 992\n",
      "####Done page 993\n",
      "####Done page 994\n",
      "####Done page 995\n",
      "####Done page 996\n",
      "####Done page 997\n",
      "####Done page 998\n",
      "####Done page 999\n",
      "####Done page 1000\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store links\n",
    "LIST_LINK_PRODUCT = []\n",
    "\n",
    "try:\n",
    "    for i in range(860, 1000):  # Adjust the range as needed\n",
    "        # Set up Selenium WebDriver\n",
    "        driver = webdriver.Chrome() \n",
    "\n",
    "        url = \"https://batdongsan.com.vn/ban-nha-dat-ha-noi/p\" + str(i + 1)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Get the page source\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Parse the page source with BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "        list_link = soup.findAll('a', 'js__product-link-for-product-id')\n",
    "\n",
    "        if not list_link:\n",
    "            print(f'####Error: No links found on page {i + 1}')\n",
    "            driver.quit()  # Quit the WebDriver if no links found\n",
    "            continue\n",
    "\n",
    "        # Extract links and append them to the list\n",
    "        for item in list_link:\n",
    "            link_href = item.get('href')\n",
    "            LIST_LINK_PRODUCT.append({'link': 'https://batdongsan.com.vn/' + link_href})\n",
    "\n",
    "        print(f'####Done page {i + 1}')\n",
    "\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'####Error getting links from page {i + 1}')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(LIST_LINK_PRODUCT)\n",
    "# df = pd.DataFrame(LIST_LINK_PRODUCT)\n",
    "# df.to_csv('data/link_data.csv')\n",
    "\n",
    "df = pd.DataFrame(LIST_LINK_PRODUCT)\n",
    "existing_df = pd.read_csv('data/link_data.csv')\n",
    "last_index = existing_df.index.max()\n",
    "df.index = range(last_index + 1, last_index + 1 + len(df))\n",
    "df.to_csv('data/link_data.csv', mode='a', header=False, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ward': 'Phường Láng Thượng', 'district': 'Đống Đa', 'price_per_sqm': '~146,25 triệu/m²', 'posted_date': '13/05/2024', 'area': '40 m²', 'price': '5,85 tỷ', 'legal_document': '', 'interior': '', 'num_bedrooms': '4 phòng', 'num_bathrooms': '', 'num_floors': '4 tầng', 'house_orientation': '', 'balcony_orientation': '', 'entrance': '', 'frontage': ''}\n"
     ]
    }
   ],
   "source": [
    "# Set up Selenium WebDriver\n",
    "driver = webdriver.Chrome() \n",
    "\n",
    "url = \"https://batdongsan.com.vn/ban-nha-rieng-duong-nguyen-chi-thanh-phuong-lang-thuong/ban-ngo-76-45m-4-tang-5-45-ty-dong-da-pr39837906\"\n",
    "driver.get(url)\n",
    "\n",
    "# Initialize a dictionary to store property details with default values\n",
    "property_details = {\n",
    "    'ward': '',\n",
    "    'district': '',\n",
    "    'price_per_sqm': '',\n",
    "    'posted_date': '',\n",
    "    'area': '',\n",
    "    'price': '',\n",
    "    'legal_document': '',\n",
    "    'interior': '',\n",
    "    'num_bedrooms': '',\n",
    "    'num_bathrooms': '',\n",
    "    'num_floors': '',\n",
    "    'house_orientation': '',\n",
    "    'balcony_orientation': '',\n",
    "    'entrance': '',\n",
    "    'frontage': ''\n",
    "}\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "with webdriver.Chrome() as driver:\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Get the page source\n",
    "    page_source = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "with open('data/test.html', 'w') as file:\n",
    "    file.write(soup.prettify())\n",
    "\n",
    "# Split the address into parts\n",
    "address_parts = soup.find('span', class_='re__pr-short-description').text.split(', ')\n",
    "\n",
    "\n",
    "# Extract ward and district\n",
    "ward = address_parts[-3]\n",
    "property_details['ward'] = ward\n",
    "district = address_parts[-2]\n",
    "property_details['district'] = district\n",
    "\n",
    "# Find the span containing the price per square meter\n",
    "price_per_sqm_span = soup.find('span', class_='ext')\n",
    "\n",
    "# Extract the text from the span\n",
    "price_per_sqm = price_per_sqm_span.text.strip()\n",
    "\n",
    "property_details['price_per_sqm'] = price_per_sqm\n",
    "\n",
    "specs_content = soup.findAll('div', 're__pr-specs-content-item')\n",
    "\n",
    "# Find the span containing the posted date\n",
    "posted_date_span = soup.find('span', string='Ngày đăng')\n",
    "\n",
    "# Extract the value of the posted date\n",
    "posted_date = posted_date_span.find_next_sibling('span').text.strip()\n",
    "property_details['posted_date'] = posted_date\n",
    "\n",
    "\n",
    "# Mapping of titles to corresponding keys in property_details\n",
    "title_map = {\n",
    "    'Diện tích': 'area',\n",
    "    'Mức giá': 'price',\n",
    "    'Giấy tờ phjáp lý': 'legal_document',\n",
    "    'Nội thất': 'interior',\n",
    "    'Số phòng ngủ': 'num_bedrooms',\n",
    "    'Số phòng tắm, vệ sinh': 'num_bathrooms',\n",
    "    'Số tầng': 'num_floors',\n",
    "    'Hướng nhà': 'house_orientation',\n",
    "    'Hướng ban công': 'balcony_orientation',\n",
    "    'Đường vào': 'entrance',\n",
    "    'Mặt tiền': 'frontage'\n",
    "}\n",
    "\n",
    "# Extract and map property details\n",
    "for item in specs_content:\n",
    "    title = item.find(class_='re__pr-specs-content-item-title').text.strip()\n",
    "    value = item.find(class_='re__pr-specs-content-item-value').text.strip()\n",
    "    if title in title_map:\n",
    "        property_details[title_map[title]] = value\n",
    "\n",
    "print(property_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  20000 non-null  int64 \n",
      " 1   link        20000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 312.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_link_data = pd.read_csv('data/link_data.csv')\n",
    "df_link_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "LIST_LINK_PRODUCT = df_link_data['link'].values.tolist()\n",
    "print(len(LIST_LINK_PRODUCT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning URL 1/20000: https://batdongsan.com.vn/ban-nha-biet-thu-lien-ke-duong-le-trong-tan-phuong-duong-noi-prj-khu-do-thi-geleximco-le-trong-tan/chinh-chu-ban-can-dep-c49-o20-kdt-geximco-80m-4-g-hoan-thien-vip-pr39840842\n",
      "Extracted details: {'type_estate': 'Nhà biệt thự, liền kề', 'district': 'Hà Đông', 'price_per_sqm': '~173,12 triệu/m²', 'posted_date': '13/05/2024', 'area': '80 m²', 'price': '13,85 tỷ', 'legal_document': 'Sổ đỏ/ Sổ hồng', 'interior': 'Đầy đủ', 'num_bedrooms': '6 phòng', 'num_bathrooms': '4 phòng', 'num_floors': '4 tầng', 'house_orientation': '', 'balcony_orientation': '', 'entrance': '', 'frontage': ''}\n",
      "Scanning URL 2/20000: https://batdongsan.com.vn/ban-shophouse-nha-pho-thuong-mai-duong-tran-phu-phuong-mo-lao/cc-chuyen-nhuong-625m-san-kinh-doanh-mat-ha-dong-so-do-lau-dai-23-ty-pr39824404\n",
      "Extracted details: {'type_estate': 'Shophouse, nhà phố thương mại', 'district': 'Hà Đông', 'price_per_sqm': '~36,8 triệu/m²', 'posted_date': '11/05/2024', 'area': '625 m²', 'price': '23 tỷ', 'legal_document': 'Sổ đỏ/ Sổ hồng', 'interior': 'Đầy đủ', 'num_bedrooms': '13 phòng', 'num_bathrooms': '12 phòng', 'num_floors': '', 'house_orientation': '', 'balcony_orientation': '', 'entrance': '40 m', 'frontage': '10 m'}\n",
      "Scanning URL 3/20000: https://batdongsan.com.vn/ban-nha-rieng-duong-nguyen-chi-thanh-phuong-lang-thuong/ban-ngo-76-45m-4-tang-5-45-ty-dong-da-pr39837906\n",
      "Extracted details: {'type_estate': 'Nhà riêng', 'district': 'Đống Đa', 'price_per_sqm': '~146,25 triệu/m²', 'posted_date': '13/05/2024', 'area': '40 m²', 'price': '5,85 tỷ', 'legal_document': 'Sổ đỏ/ Sổ hồng', 'interior': '', 'num_bedrooms': '4 phòng', 'num_bathrooms': '3 phòng', 'num_floors': '4 tầng', 'house_orientation': '', 'balcony_orientation': '', 'entrance': '', 'frontage': ''}\n",
      "Scanning URL 4/20000: https://batdongsan.com.vn/ban-nha-mat-pho-duong-nguyen-duc-canh-phuong-hoang-van-thu-4/ban-co-via-he-dien-tich-220-8-m2-gia-158-tr-m2-pr39832145\n"
     ]
    }
   ],
   "source": [
    "LIST_PRODUCT = []\n",
    "\n",
    "# Define a function to extract property details\n",
    "def extract_property_details(url):\n",
    "    property_details = {\n",
    "        'type_estate': '',\n",
    "        'district': '',\n",
    "        'price_per_sqm': '',\n",
    "        'posted_date': '',\n",
    "        'area': '',\n",
    "        'price': '',\n",
    "        'legal_document': '',\n",
    "        'interior': '',\n",
    "        'num_bedrooms': '',\n",
    "        'num_bathrooms': '',\n",
    "        'num_floors': '',\n",
    "        'house_orientation': '',\n",
    "        'balcony_orientation': '',\n",
    "        'entrance': '',\n",
    "        'frontage': ''\n",
    "    }\n",
    "\n",
    "    # Set up Selenium WebDriver\n",
    "    with webdriver.Chrome() as driver:\n",
    "        driver.get(url)\n",
    "        # Get the page source\n",
    "        page_source = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "    # Find the breadcrumb element\n",
    "    breadcrumb = soup.find('div', class_='re__breadcrumb')\n",
    "\n",
    "    level_4_link = breadcrumb.find('a', {'level': '4'})\n",
    "    level_4_title = level_4_link.text\n",
    "\n",
    "    # Use regular expressions to extract the type_estate\n",
    "    match = re.search(r\"(.+?) tại\", level_4_title)\n",
    "    type_estate = match.group(1) if match else \"\"\n",
    "    property_details['type_estate'] = type_estate\n",
    "\n",
    "    level_3_link = breadcrumb.find('a', {'level': '3'})\n",
    "    property_details['district'] = level_3_link.text\n",
    "\n",
    "    # Extract price per square meter\n",
    "    price_per_sqm_span = soup.find('span', class_='ext')\n",
    "    if price_per_sqm_span:\n",
    "        property_details['price_per_sqm'] = price_per_sqm_span.text.strip()\n",
    "\n",
    "    # Extract posted date\n",
    "    posted_date_span = soup.find('span', string='Ngày đăng')\n",
    "    property_details['posted_date'] = posted_date_span.find_next_sibling('span').text.strip()\n",
    "\n",
    "    # Extract property specifications\n",
    "    specs_content = soup.find_all('div', class_='re__pr-specs-content-item')\n",
    "\n",
    "    # Mapping of titles to corresponding keys in property_details\n",
    "    title_map = {\n",
    "        'Diện tích': 'area',\n",
    "        'Mức giá': 'price',\n",
    "        'Pháp lý': 'legal_document',\n",
    "        'Nội thất': 'interior',\n",
    "        'Số phòng ngủ': 'num_bedrooms',\n",
    "        'Số toilet': 'num_bathrooms',\n",
    "        'Số tầng': 'num_floors',\n",
    "        'Hướng nhà': 'house_orientation',\n",
    "        'Hướng ban công': 'balcony_orientation',\n",
    "        'Đường vào': 'entrance',\n",
    "        'Mặt tiền': 'frontage'\n",
    "    }\n",
    "\n",
    "    # Extract and map property details\n",
    "    for item in specs_content:\n",
    "        title = item.find(class_='re__pr-specs-content-item-title').text.strip()\n",
    "        if title in title_map:\n",
    "            property_details[title_map[title]] = item.find(class_='re__pr-specs-content-item-value').text.strip()\n",
    "    \n",
    "    return property_details\n",
    "\n",
    "# Iterate through LIST_LINK_PRODUCT and append results to LIST_PRODUCT\n",
    "for index, link in enumerate(LIST_LINK_PRODUCT):\n",
    "    try:\n",
    "        print(f\"Scanning URL {index + 1}/{len(LIST_LINK_PRODUCT)}: {link}\")\n",
    "        details = extract_property_details(link)\n",
    "        LIST_PRODUCT.append(details)\n",
    "        print(f\"Extracted details: {details}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract data for {link}: {e}\")\n",
    "        break  # Stop further processing to check the issue\n",
    "\n",
    "# Now LIST_PRODUCT contains the extracted details for all successfully processed URLs\n",
    "print(LIST_PRODUCT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "print(len(LIST_PRODUCT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link_product = pd.DataFrame(LIST_PRODUCT)\n",
    "df_link_product.to_csv('data/raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"data/raw_data.csv\")\n",
    "\n",
    "# Drop the 'link' and 'describe' columns\n",
    "df.drop(['link', 'describe'], axis=1, inplace=True)\n",
    "\n",
    "# Save the modified DataFrame back to a CSV file\n",
    "df.to_csv(\"data/raw_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Unnamed: 0    4000 non-null   int64  \n",
      " 1   estate_type   3210 non-null   object \n",
      " 2   province      3210 non-null   object \n",
      " 3   district      3206 non-null   object \n",
      " 4   ward          3206 non-null   object \n",
      " 5   price         3824 non-null   object \n",
      " 6   square        3770 non-null   object \n",
      " 7   post_date     4000 non-null   object \n",
      " 8   numb_bedroom  3076 non-null   float64\n",
      " 9   numb_toilet   2864 non-null   float64\n",
      " 10  numb_floor    3155 non-null   float64\n",
      "dtypes: float64(3), int64(1), object(7)\n",
      "memory usage: 343.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/raw_data.csv')\n",
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
